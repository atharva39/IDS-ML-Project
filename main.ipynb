{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Preprocessing \n",
    "Importing libraries and reading features list from 'kddcup.names.txt' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# reading features list\n",
    "with open(\"dataset/kddcup.names.txt\", 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appending columns to the dataset and adding a new column name 'target' to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=\"\"\"\n",
    "duration,\n",
    "protocol_type,\n",
    "service,\n",
    "flag,\n",
    "src_bytes,\n",
    "dst_bytes,\n",
    "land,\n",
    "wrong_fragment,\n",
    "urgent,\n",
    "hot,\n",
    "num_failed_logins,\n",
    "logged_in,\n",
    "num_compromised,\n",
    "root_shell,\n",
    "su_attempted,\n",
    "num_root,\n",
    "num_file_creations,\n",
    "num_shells,\n",
    "num_access_files,\n",
    "num_outbound_cmds,\n",
    "is_host_login,\n",
    "is_guest_login,\n",
    "count,\n",
    "srv_count,\n",
    "serror_rate,\n",
    "srv_serror_rate,\n",
    "rerror_rate,\n",
    "srv_rerror_rate,\n",
    "same_srv_rate,\n",
    "diff_srv_rate,\n",
    "srv_diff_host_rate,\n",
    "dst_host_count,\n",
    "dst_host_srv_count,\n",
    "dst_host_same_srv_rate,\n",
    "dst_host_diff_srv_rate,\n",
    "dst_host_same_src_port_rate,\n",
    "dst_host_srv_diff_host_rate,\n",
    "dst_host_serror_rate,\n",
    "dst_host_srv_serror_rate,\n",
    "dst_host_rerror_rate,\n",
    "dst_host_srv_rerror_rate\n",
    "\"\"\"\n",
    "\n",
    "columns = []\n",
    "for c in cols.split(',\\n'):\n",
    "    if (c.strip()):\n",
    "        columns.append(c.strip())\n",
    "        \n",
    "columns.append('target')\n",
    "print(len(columns)) # Output will be 41 + 1 = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the 'training_attack_types.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dataset/training_attack_types.txt\", 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a dictionary of 'attack_types'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_types = {\n",
    "    'normal': 'normal',\n",
    "    'back': 'dos',\n",
    "    'buffer_overflow': 'u2r',\n",
    "    'ftp_write': 'r2l',\n",
    "    'guess_passwd': 'r2l',\n",
    "    'imap': 'r2l',\n",
    "    'ipsweep': 'probe',\n",
    "    'land': 'dos',\n",
    "    'loadmodule': 'u2r',\n",
    "    'multihop': 'r2l',\n",
    "    'neptune': 'dos',\n",
    "    'nmap': 'probe',\n",
    "    'perl': 'u2r',\n",
    "    'phf': 'r2l',\n",
    "    'pod': 'dos',\n",
    "    'portsweep': 'probe',\n",
    "    'rootkit': 'u2r',\n",
    "    'satan': 'probe',\n",
    "    'smurf': 'dos',\n",
    "    'spy': 'r2l',\n",
    "    'teardrop': 'dos',\n",
    "    'warezclient': 'r2l',\n",
    "    'warezmaster': 'r2l',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Reading the dataset (’kddcup.data.gz’) and adding Attack Type feature in the training dataset where attack type feature has 5 distinct values i.e. dos, normal, probe, r2l, u2r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset/kddcup.data.gz\"\n",
    "df = pd.read_csv(path, names = columns)\n",
    "\n",
    "# Adding Attack Type Column\n",
    "df[\"Attack Type\"] = df.target.apply(lambda r:attack_types[r[:-1]])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shape of dataframe and getting data type of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "# no missing values found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding categorical features\n",
    "num_cols = df._get_numeric_data().columns\n",
    "\n",
    "cate_cols = list(set(df.columns)-set(num_cols))\n",
    "cate_cols.remove('target')\n",
    "cate_cols.remove('Attack Type')\n",
    "\n",
    "cate_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Visualization: Protocol Type Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "protocol_counts = df['protocol_type'].value_counts() / 100000  # Convert to Lakhs\n",
    "protocol_counts.plot(kind='bar', color='darkblue')\n",
    "plt.title('Protocol Type Distribution (in Lakhs)')\n",
    "plt.xlabel('Protocol Type')\n",
    "plt.ylabel('Count (Lakhs)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Visualization: Logged in Success Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "logged_in_counts = df['logged_in'].value_counts() / 100000  # Convert to Lakhs\n",
    "logged_in_counts.plot(kind='bar', color=['blue', 'green'])\n",
    "plt.title('Logged In Success Rate (in Lakhs)')\n",
    "plt.xlabel('Logged In (1=Yes, 0=No)')\n",
    "plt.ylabel('Number of Packets (Lakhs)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Visualization: Attack Type Distributio in Lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "attack_counts = df['Attack Type'].value_counts() / 100000  # Convert to Lakhs\n",
    "attack_counts.plot(kind='bar', color='purple')\n",
    "plt.title('Attack Type Distribution (in Lakhs)')\n",
    "plt.xlabel('Attack Type')\n",
    "plt.ylabel('Frequency (Lakhs)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Correlation - Finding the highly correlated variables using heatmap and ignore them for analysis  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with NaN\n",
    "df = df.dropna(axis='columns')\n",
    "\n",
    "# Keep columns where there are more than 1 unique values and are numeric\n",
    "ndf = df[[col for col in df.columns if df[col].nunique() > 1 and pd.api.types.is_numeric_dtype(df[col])]]\n",
    "\n",
    "# Now calculate the correlation matrix\n",
    "corr = ndf.corr()\n",
    "\n",
    "plt.figure(figsize = (15, 12))\n",
    "sns.heatmap(corr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing highly correlated features from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable is highly correlated with num_compromised and should be ignored for analysis.\n",
    "#(Correlation = 0.9938277978738366)\n",
    "df.drop('num_root', axis = 1, inplace = True)\n",
    "\n",
    "# This variable is highly correlated with serror_rate and should be ignored for analysis.\n",
    "#(Correlation = 0.9983615072725952)\n",
    "df.drop('srv_serror_rate', axis = 1, inplace = True)\n",
    "\n",
    "# This variable is highly correlated with rerror_rate and should be ignored for analysis.\n",
    "#(Correlation = 0.9947309539817937)\n",
    "df.drop('srv_rerror_rate', axis = 1, inplace = True)\n",
    "\n",
    "# This variable is highly correlated with srv_serror_rate and should be ignored for analysis.\n",
    "#(Correlation = 0.9993041091850098)\n",
    "df.drop('dst_host_srv_serror_rate', axis = 1, inplace = True)\n",
    "\n",
    "# This variable is highly correlated with rerror_rate and should be ignored for analysis.\n",
    "#(Correlation = 0.9869947924956001)\n",
    "df.drop('dst_host_serror_rate', axis = 1, inplace = True)\n",
    "\n",
    "# This variable is highly correlated with srv_rerror_rate and should be ignored for analysis.\n",
    "#(Correlation = 0.9821663427308375)\n",
    "df.drop('dst_host_rerror_rate', axis = 1, inplace = True)\n",
    "\n",
    "# This variable is highly correlated with rerror_rate and should be ignored for analysis.\n",
    "#(Correlation = 0.9851995540751249)\n",
    "df.drop('dst_host_srv_rerror_rate', axis = 1, inplace = True)\n",
    "\n",
    "# This variable is highly correlated with srv_rerror_rate and should be ignored for analysis.\n",
    "#(Correlation = 0.9865705438845669)\n",
    "df.drop('dst_host_same_srv_rate', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Mapping – Apply feature mapping on features such as : ‘protocol_type’ & ‘flag’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# protocol_type feature mapping\n",
    "pmap = {'icmp':0, 'tcp':1, 'udp':2}\n",
    "df['protocol_type'] = df['protocol_type'].map(pmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag feature mapping\n",
    "fmap = {'SF':0, 'S0':1, 'REJ':2, 'RSTR':3, 'RSTO':4, 'SH':5, 'S1':6, 'S2':7, 'RSTOS0':8, 'S3':9, 'OTH':10}\n",
    "df['flag'] = df['flag'].map(fmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove irrelevant features such as ‘service’ before modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"service\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 – Modelling\n",
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting features and target variables with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "df = df.drop(['target', ], axis = 1)\n",
    "print(df.shape)\n",
    "\n",
    "# Target variable and train set\n",
    "y = df[['Attack Type']]\n",
    "X = df.drop(['Attack Type', ], axis = 1)\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "# Split test and train data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.33, random_state= 42)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying various machine learning classification algorithms such as Support Vector Machines, Random Forest, Naive Bayes, Decision Tree, Logistic Regression to create different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python implementation of Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model using Gaussian Naive Bayes on X_train and y_train\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clfg = GaussianNB()\n",
    "start_time = time.time()\n",
    "clfg.fit(X_train, y_train.values.ravel())\n",
    "end_time = time.time()\n",
    "gnb_train_time = end_time - start_time # Created for visualizing comfort\n",
    "\n",
    "print(\"Training Time: \", gnb_train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Gaussian Naive Bayes model by making predictions on the test data\n",
    "start_time = time.time()\n",
    "y_test_pred = clfg.predict(X_test) # Change the X_test to test using test data \n",
    "end_time = time.time()\n",
    "gnb_test_time = end_time - start_time # Created for visualizing comfort\n",
    "\n",
    "print(\"Testing Time: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfg_train_score = clfg.score(X_train, y_train)\n",
    "print(\"Train score is:\", clfg_train_score)\n",
    "clfg_test_score = clfg.score(X_test, y_test)\n",
    "print(\"Test score is:\", clfg_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python implementation of Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Decision Tree Classifier using entropy criterion and a maximum depth of 4, while measuring the training time.\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clfd = DecisionTreeClassifier(criterion = \"entropy\", max_depth= 4)\n",
    "start_time = time.time()\n",
    "clfd.fit(X_train, y_train.values.ravel())\n",
    "end_time = time.time()\n",
    "dt_train_time = end_time - start_time # Created for visualizing comfort\n",
    "\n",
    "print(\"Training Time: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class labels for the training data and measure the testing time.\n",
    "\n",
    "start_time = time.time()\n",
    "y_test_pred = clfd.predict(X_test) # Change the X_test to test using test data \n",
    "end_time = time.time()\n",
    "dt_test_time = end_time - start_time # Created for visualizing comfort\n",
    "\n",
    "print(\"Testing Time: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfd_train_score = clfd.score(X_train, y_train)\n",
    "print(\"Train score is:\", clfd_train_score)\n",
    "clfd_test_score = clfd.score(X_test, y_test)\n",
    "print(\"Test score is:\", clfd_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python code implementation of Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest Classifier with 30 estimators and measure the training time.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clfr = RandomForestClassifier(n_estimators= 30)\n",
    "start_time = time.time()\n",
    "clfr.fit(X_train, y_train.values.ravel())\n",
    "end_time = time.time()\n",
    "rf_train_time = end_time - start_time # Created for visualizing comfort\n",
    "\n",
    "print(\"Training Time: \", end_time - start_time)\n",
    "\n",
    "# Training time took 180 seconds for the first time.\n",
    "# Training time took 175 seconds for the second time.\n",
    "# Training time took 152 seconds for the third time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "y_test_pred = clfr.predict(X_test)\n",
    "end_time = time.time()\n",
    "rf_test_time = end_time - start_time # Created for visualizing comfort\n",
    "\n",
    "print(\"Testing time: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfr_train_score = clfr.score(X_train, y_train)\n",
    "print(\"Train score is:\", clfr_train_score)\n",
    "clfr_test_score = clfr.score(X_test, y_test)\n",
    "print(\"Test score is:\", clfr_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python implementation of Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clfs = LinearSVC()\n",
    "start_time = time.time()\n",
    "clfs.fit(X_train, y_train.values.ravel())\n",
    "end_time = time.time()\n",
    "svc_train_time = end_time - start_time # Created for visualizing comfort\n",
    "\n",
    "print(\"Training Time: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class labels for the training data and measure the testing time.\n",
    "\n",
    "start_time = time.time()\n",
    "y_test_pred = clfs.predict(X_test)\n",
    "end_time = time.time()\n",
    "svc_test_time = end_time - start_time # Created for visualizing comfort\n",
    "\n",
    "print(\"Testing Time: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs_train_score = clfs.score(X_train, y_train)\n",
    "print(\"Train score is:\", clfs_train_score)\n",
    "clfs_test_score = clfs.score(X_test, y_test)\n",
    "print(\"Test score is:\", clfs_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python implementation of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression model on the training data with a high iteration limit to ensure convergence\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clfl = LogisticRegression(max_iter= 1200000)\n",
    "start_time = time.time()\n",
    "clfl.fit(X_train, y_train.values.ravel())\n",
    "end_time = time.time()\n",
    "lr_train_time = end_time - start_time # Created for visualizing comfort\n",
    "\n",
    "print(\"Training Time: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class labels for the test data using the trained Logistic Regression model and measure the prediction time\n",
    "\n",
    "start_time = time.time()\n",
    "y_test_pred = clfl.predict(X_test)\n",
    "end_time = time.time()\n",
    "lr_test_time = end_time - start_time # Created for visualizing comfort\n",
    "\n",
    "print(\"Testing Time: \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfl_train_score = clfl.score(X_train, y_train)\n",
    "print(\"Train score is:\", clfl_train_score)\n",
    "clfl_test_score = clfl.score(X_test, y_test)\n",
    "print(\"Test score is:\", clfl_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python implementation of Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Convert X_train to DataFrame if it's a NumPy array\n",
    "if isinstance(X_train, np.ndarray):\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "\n",
    "# Convert y_train to Series if it's a NumPy array\n",
    "if isinstance(y_train, np.ndarray):\n",
    "    y_train = pd.Series(y_train)\n",
    "\n",
    "# Randomly sample 10% of the training data\n",
    "X_train_sampled = X_train.sample(frac=0.1, random_state=1)  # 10% of data\n",
    "\n",
    "# Ensure y_train corresponds to the sampled X_train using random sampling\n",
    "y_train_sampled = y_train.sample(frac=0.1, random_state=1)  # Sample the same percentage\n",
    "\n",
    "# Convert y_train_sampled to 1D using .ravel()\n",
    "y_train_sampled = y_train_sampled.values.ravel()  # This avoids the warning\n",
    "\n",
    "# Train the Gradient Boosting Classifier on the sampled data\n",
    "clfgd = GradientBoostingClassifier(n_estimators=50, max_depth=3, subsample=0.5, random_state=0)\n",
    "start_time = time.time()\n",
    "clfgd.fit(X_train_sampled, y_train_sampled)  # Train on the sampled data\n",
    "end_time = time.time()\n",
    "gd_train_time = end_time - start_time # Created for visualizing comfort\n",
    "\n",
    "# Output the training time\n",
    "print(\"Training Time: \", end_time - start_time)\n",
    "\n",
    "\n",
    "#########################################\n",
    "# Beep sound after executing the cell\n",
    "#########################################\n",
    "import winsound\n",
    "\n",
    "print(\"Code is completed running\")\n",
    "\n",
    "# Beep sound after the code finishes\n",
    "winsound.Beep(1000, 1000)  # Frequency: 1000 Hz, Duration: 1000 ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "y_test_pred = clfgd.predict(X_test)\n",
    "end_time = time.time()\n",
    "gd_test_time = end_time - start_time # Created for visualizing comfort\n",
    "\n",
    "print(\"Testing time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfgd_train_score = clfgd.score(X_train, y_train)\n",
    "print(\"Train score is:\", clfgd_train_score)\n",
    "clfgd_test_score = clfgd.score(X_test, y_test)\n",
    "print(\"Test score is:\", clfgd_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "In this section, we will evaluate the performance of our trained models using accuracy, precision, recall, F1 score, and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for metrics calculation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define a function to calculate and return the metrics\n",
    "def evaluate_model(y_test, y_test_pred, model_name):\n",
    "    print(f\"--- {model_name} Evaluation ---\")\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Precision\n",
    "    precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    \n",
    "    # Recall\n",
    "    recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    \n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    print(\"\\n\")  # New line for readability\n",
    "    \n",
    "    # Return the metrics as a dictionary\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1\n",
    "    }\n",
    "\n",
    "# Make predictions for each model\n",
    "nb_predictions = clfg.predict(X_test)  # For Gaussian Naive Bayes\n",
    "dt_predictions = clfd.predict(X_test)    # For Decision Tree\n",
    "rf_predictions = clfr.predict(X_test)    # For Random Forest\n",
    "svc_predictions = clfs.predict(X_test)   # For Support Vector Classifier\n",
    "lr_predictions = clfl.predict(X_test)     # For Logistic Regression\n",
    "gd_predictions = clfgd.predict(X_test)    # For Gradient Descent\n",
    "\n",
    "# Call the function for each model and store the results\n",
    "nb_metrics = evaluate_model(y_test, nb_predictions, \"Gaussian Naive Bayes\")\n",
    "dt_metrics = evaluate_model(y_test, dt_predictions, \"Decision Tree\")\n",
    "rf_metrics = evaluate_model(y_test, rf_predictions, \"Random Forest\")\n",
    "svc_metrics = evaluate_model(y_test, svc_predictions, \"Support Vector Classifier\")\n",
    "lr_metrics = evaluate_model(y_test, lr_predictions, \"Logistic Regression\")\n",
    "gd_metrics = evaluate_model(y_test, gd_predictions, \"Gradient Descent\")\n",
    "\n",
    "# Now you can access metrics like this\n",
    "print(nb_metrics['accuracy'])  # Access Gaussian Naive Bayes accuracy\n",
    "\n",
    "##### This will take 7 minutes to complete #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing Accuracy and F1 Score metrics with each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model names and their corresponding metrics\n",
    "model_names = ['NB', 'DT', 'RF', 'SVM', 'LR', 'GD']\n",
    "\n",
    "# Model names and their corresponding metrics\n",
    "model_names = ['NB', 'DT', 'RF', 'SVM', 'LR', 'GD']\n",
    "\n",
    "# Accuracy values for all models (converted to percentage)\n",
    "accuracy_values = [nb_metrics['accuracy'] * 100, \n",
    "                   dt_metrics['accuracy'] * 100, \n",
    "                   rf_metrics['accuracy'] * 100, \n",
    "                   svc_metrics['accuracy'] * 100, \n",
    "                   lr_metrics['accuracy'] * 100, \n",
    "                   gd_metrics['accuracy'] * 100]\n",
    "\n",
    "# F1 Score values for all models\n",
    "f1_values = [nb_metrics['f1_score'], \n",
    "            dt_metrics['f1_score'], \n",
    "            rf_metrics['f1_score'], \n",
    "            svc_metrics['f1_score'], \n",
    "            lr_metrics['f1_score'], \n",
    "            gd_metrics['f1_score']]\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "axs[0].bar(model_names, accuracy_values)\n",
    "axs[0].set_title('Model Accuracy Comparison')\n",
    "axs[0].set_ylabel('Accuracy (%)')\n",
    "axs[0].set_ylim(0, 100)  # Set y-axis limit for accuracy\n",
    "\n",
    "# Plot F1 Score\n",
    "axs[1].bar(model_names, f1_values, color='green')\n",
    "axs[1].set_title('Model F1 Score Comparison')\n",
    "axs[1].set_ylabel('F1 Score')\n",
    "axs[1].set_ylim(0, 1)  # Set y-axis limit for F1 Score\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot accuracy\n",
    "axs[0].bar(model_names, accuracy_values)\n",
    "axs[0].set_title('Model Accuracy Comparison')\n",
    "axs[0].set_ylabel('Accuracy (%)')\n",
    "axs[0].set_ylim(0, 100)  # Set y-axis limit for accuracy\n",
    "\n",
    "# Plot F1 Score\n",
    "axs[1].bar(model_names, f1_values, color='green')\n",
    "axs[1].set_title('Model F1 Score Comparison')\n",
    "axs[1].set_ylabel('F1 Score')\n",
    "axs[1].set_ylim(0, 1)  # Set y-axis limit for F1 Score\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze the training and testing accuracy of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training accuracy of each model\n",
    "\n",
    "names = ['NB', 'DT', 'RF', 'SVM', 'LR', 'GD']\n",
    "values = [clfg_train_score, clfd_train_score, clfr_train_score, clfs_train_score, clfl_train_score, clfgd_train_score]\n",
    "\n",
    "# Create a figure\n",
    "f = plt.figure(figsize=(15, 3), num=10)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.subplot(131)\n",
    "plt.bar(names, values)\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Training Accuracy of Each Model')\n",
    "plt.xlabel('Model Names')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing accuracy of each model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "names = ['NB', 'DT', 'RF', 'SVM', 'LR', 'GD']\n",
    "values = [clfg_test_score, clfd_test_score, clfr_test_score, clfs_test_score, clfl_test_score, clfgd_test_score]\n",
    "\n",
    "# Create a figure\n",
    "f = plt.figure(figsize=(15, 3), num=10)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.subplot(131)\n",
    "plt.bar(names, values)\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Testing Accuracy of Each Model')\n",
    "plt.xlabel('Model Names')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the training and testing time of each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time of each model in seconds\n",
    "\n",
    "names = ['NB', 'DT', 'RF', 'SVC', 'LR', 'GD']\n",
    "values = [gnb_train_time, dt_train_time, rf_train_time, svc_train_time, lr_train_time, gd_train_time]\n",
    "\n",
    "# Create a figure\n",
    "f = plt.figure(figsize=(15, 3), num=10)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.subplot(131)\n",
    "plt.bar(names, values)\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Training Time of Each Model')\n",
    "plt.xlabel('Model Names')\n",
    "plt.ylabel('Training Time (seconds)')\n",
    "\n",
    "# Set y-axis limit to 100 seconds\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing time of each model in seconds\n",
    "names = ['NB', 'DT', 'RF', 'SVM', 'LR', 'GD']\n",
    "values = [gnb_test_time, dt_test_time, rf_test_time, svc_test_time, lr_test_time, gd_test_time]\n",
    "\n",
    "# Create a figure\n",
    "f = plt.figure(figsize=(15, 3), num=10)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.subplot(131)\n",
    "plt.bar(names, values)\n",
    "\n",
    "# Adding title and labels\n",
    "plt.title('Testing Time of Each Model')\n",
    "plt.xlabel('Model Names')\n",
    "plt.ylabel('Testing Time (seconds)')\n",
    "\n",
    "# Set y-axis limit to 4 seconds\n",
    "plt.ylim(0, 4)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
